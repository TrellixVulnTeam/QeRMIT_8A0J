********************************useful commands***************************************
scp -r  boss@40.80.149.205:/tmp/debug_squad/ /home/mrkchang/Documents/Stanford/CS224N/fromVM

git config --global credential.helper cache
git config --global credential.helper 'cache --timeout=7200'

nvidia-smi
tmux new -s [name]
ctrl - b - [
tmux list-sessions
^b = cntrl-b
cntrl - b - d
$ tmux a -t [name]
$ tmux a -t #

sudo prime-select intel
sudo prime-select nvidia
prime-select query

rm = remove CAREFUL!!
mv = move/rename
***********************************************************************
python train.py   --bert_model bert-base-uncased   --do_train   --do_predict   --do_lower_case   --train_file ./data/train-v2.0.json   --predict_file ./data/dev-v2.0.json   --train_batch_size 12   --learning_rate 3e-5   --num_train_epochs 2.0   --max_seq_length 384   --doc_stride 128   --output_dir ./tmp/debug_squad/ --version_2_with_negative





data_path (str): Path to .npz file containing pre-processed dataset.
dataset = np.load(data_path)
self.ids = torch.from_numpy(dataset['ids']).long()

python test_bidaf.py --split dev --load_path save/train/baseline-01/step_1951227.pth.tar  --name dbging

python run_squad.py   --bert_model bert-base-uncased   --do_train   --do_predict   --do_lower_case   --train_file ./data/train-v2.0.json   --predict_file ./data/dev-v2.0.json   --train_batch_size 24   --learning_rate 3e-5   --num_train_epochs 2.0   --max_seq_length 384   --doc_stride 128   --output_dir /tmp/debug_squad_redo_from_run_squad/ --version_2_with_negative --fp16 --loss_scale 128


#remember
scp -r /home/mrkchang/Documents/Stanford/CS224N/cs224n-project/data  boss@40.80.149.205:~/hugface/hugface/pytorch-pretrained-BERT/

#transfer from vm to my local laptop model parameters
scp -r boss@13.77.171.174:~/models_squad/ /Users/diai/Documents/CS_224N_NLP/Project_Local/models_squad

scp -r boss@40.80.149.205:~/models_squad/debug_squad_baseline_03092019_cls /Users/diai/Documents/CS_224N_NLP/Project_Local/models_squad/


#diai local export functions
export squad_dir=~/Documents/CS_224N_NLP/Project/QeRMIT/pytorch-pretrained-BERT/data
export model_dir=~/Documents/CS_224N_NLP/Project_Local/models_squad/models_squad
export output_dir=~/Documents/CS_224N_NLP/Project_Local/models_squad/models_squad/debug_squad_baseline_03072019_cnn

#diai run on vm scripts
#cnn model
python run_squad.py --bert_model bert-large-uncased --do_train --do_predict --do_lower_case --train_file $squad_dir/train-v2.0.json --predict_file $squad_dir/dev-v2.0.json --learning_rate 3e-4 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --output_dir $output_dir/debug_squad_baseline_03082018_cnn --train_batch_size 4 --gradient_accumulation_steps 2 --fp16 --loss_scale 128 --version_2_with_negative --new_model --cache_example &> train_cnn_output.txt

#cls model
export squad_dir=~/QeRMIT/pytorch-pretrained-BERT/data
export output_dir=~/models_squad

#batch size for base and seq length 384 is 12 for 12 GB of ram. We have only 8 GB for each M60 GPU.

python run_squad.py --bert_model bert-base-uncased --do_train --do_predict --do_lower_case --train_file $squad_dir/train-v2.0.json --predict_file $squad_dir/dev-v2.0.json --learning_rate 3e-4 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --output_dir $output_dir/debug_squad_baseline_03092019_cls --train_batch_size 24 --gradient_accumulation_steps 4 --version_2_with_negative --new_model --cache_example --model3 &> train_cls_output.txt

#cls model Bert base f1 and em scores
Submission successful
Scores (Improvement): 
EM: 64.988 (-12.537) 
F1: 68.148 (-12.258)

#command to transfer files over from cns to desktop
rm -R ~/Desktop/models_squad/train_default_v2/
mkdir ~/Desktop/models_squad/train_default_v2/
fileutil cp -R /cns/os-d/home/diai/public/bert_output/squadv2.0/train_default_v2/*.csv ~/Desktop/models_squad/train_default_v2/
fileutil cp -R /cns/os-d/home/diai/public/bert_output/squadv2.0/train_default_v2/*.json ~/Desktop/models_squad/train_default_v2/
fileutil cp -R /cns/os-d/home/diai/public/bert_output/squadv2.0/train_default_v2/*tf* ~/Desktop/models_squad/train_default_v2/
#fileutil rm /cns/os-d/home/diai/public/bert_output/squadv2.0/train_default_v2/*

#transfer files from desktop to laptop
scp -r diai@diai.mtv.corp.google.com:~/Desktop/models_squad/train_default_v2/ /Users/diai/Documents/CS_224N_NLP/Project/QeRMIT/submission_files/bert_large


# Google Borg Commands to run BERT on TPU
g4d tensorflow_tpu
alias allocator_borgcfg='/google/data/ro/teams/traino/borgcfg'
blaze build --copt=-mavx --define=tpu_hardware=true -c opt experimental/users/diai/language/bert:run_squad_google.par
allocator_borgcfg --skip_confirmation experimental/users/diai/language/bert/google/train_squad_tpu.borg reload --vars=exp_name=train_default_v2,version_2_with_negative=true

# Select threshold for null versus non-null answer
alias output_dir='~/Desktop/models_squad/train_default_v2'
python ~/Desktop/lumiere/squad/evaluate-v2.0.py ~/Desktop/models_squad/data/dev-v2.0.json ~/Desktop/models_squad/train_default_v2/predictions.json --na-prob-file ~/Desktop/models_squad/train_default_v2/null_odds.json
# Get predictions with new threshold like this:
allocator_borgcfg --skip_confirmation experimental/users/diai/language/bert/google/train_squad_tpu.borg reload --vars=exp_name=train_default_v2,version_2_with_negative=true,do_train=false,null_score_diff_threshold=-5.914348542690277


# threshold scores on original dev set.
{
  "exact": 77.13299081950645, 
  "f1": 80.37971293567406, 
  "total": 11873, 
  "HasAns_exact": 76.38326585695006, 
  "HasAns_f1": 82.8860208645848, 
  "HasAns_total": 5928, 
  "NoAns_exact": 77.88057190916737, 
  "NoAns_f1": 77.88057190916737, 
  "NoAns_total": 5945, 
  "best_exact": 78.37109407900277, 
  "best_exact_thresh": -5.374411344528198, 
  "best_f1": 81.14082103554928, 
  "best_f1_thresh": -2.325418710708618
}

# threshold scores on our CS224N dev set.
{
  "exact": 77.31161566304705, 
  "f1": 80.24336841642369, 
  "total": 6078, 
  "HasAns_exact": 78.10996563573883, 
  "HasAns_f1": 84.23339973712166, 
  "HasAns_total": 2910, 
  "NoAns_exact": 76.57828282828282, 
  "NoAns_f1": 76.57828282828282, 
  "NoAns_total": 3168, 
  "best_exact": 79.13787430075683, 
  "best_exact_thresh": -5.374411344528198, 
  "best_f1": 81.59913492960295, 
  "best_f1_thresh": -5.374411344528198
}

# Threshold scores for max_seq_len 512
{
  "exact": 77.36097400460677, 
  "f1": 80.33378036270929, 
  "total": 6078, 
  "HasAns_exact": 78.10996563573883, 
  "HasAns_f1": 84.31914675070372, 
  "HasAns_total": 2910, 
  "NoAns_exact": 76.67297979797979, 
  "NoAns_f1": 76.67297979797979, 
  "NoAns_total": 3168, 
  "best_exact": 79.12142152023692, 
  "best_exact_thresh": -5.584860801696777, 
  "best_f1": 81.58470072551658, 
  "best_f1_thresh": -5.584860801696777
}

# Threshold scores for max_seq_len 512, max_ans_len and max_query_len of 256
{
  "exact": 77.27871010200724, 
  "f1": 80.27535735105508, 
  "total": 6078, 
  "HasAns_exact": 77.9381443298969, 
  "HasAns_f1": 84.19712095522796, 
  "HasAns_total": 2910, 
  "NoAns_exact": 76.67297979797979, 
  "NoAns_f1": 76.67297979797979, 
  "NoAns_total": 3168, 
  "best_exact": 79.05561039815728, 
  "best_exact_thresh": -5.584860801696777, 
  "best_f1": 81.55109051728252, 
  "best_f1_thresh": -5.584860801696777
}

# Threshold scores for optimized BERT
{
  "exact": 73.51102336294834, 
  "f1": 77.27169981723289, 
  "total": 6078, 
  "HasAns_exact": 75.56701030927834, 
  "HasAns_f1": 83.42178401688732, 
  "HasAns_total": 2910, 
  "NoAns_exact": 71.62247474747475, 
  "NoAns_f1": 71.62247474747475, 
  "NoAns_total": 3168, 
  "best_exact": 76.34090161237249, 
  "best_exact_thresh": -26.965904235839844, 
  "best_f1": 79.17271051315475, 
  "best_f1_thresh": -24.478356897830963
}

# Threshold scores for BERT + CNN without highway connection
{
  "exact": 66.46923330042777, 
  "f1": 72.67241206748663, 
  "total": 6078, 
  "HasAns_exact": 60.0, 
  "HasAns_f1": 72.95633008459943, 
  "HasAns_total": 2910, 
  "NoAns_exact": 72.41161616161617, 
  "NoAns_f1": 72.41161616161617, 
  "NoAns_total": 3168, 
  "best_exact": 68.97005593945377, 
  "best_exact_thresh": -10.598368883132935, 
  "best_f1": 74.01762203689894, 
  "best_f1_thresh": -6.945757150650024
}

# Threshold scores for BERT + CNN with highway connection
{
  "exact": 76.25863770977296, 
  "f1": 79.65622468964212, 
  "total": 6078, 
  "HasAns_exact": 77.42268041237114, 
  "HasAns_f1": 84.5190837332114, 
  "HasAns_total": 2910, 
  "NoAns_exact": 75.18939393939394, 
  "NoAns_f1": 75.18939393939394, 
  "NoAns_total": 3168, 
  "best_exact": 78.00263244488319, 
  "best_exact_thresh": -9.801985181868076, 
  "best_f1": 81.06310918389102, 
  "best_f1_thresh": -5.914348542690277
}


# BERT SCORES
# BERT LARGE
Submission successful
Scores (Improvement): 
EM: 77.312 (-0.971) 
F1: 80.243 (-0.826)

# BERT LARGE WITH NO ANSWER TUNING THEIR DEV SET
Submission successful
Scores (Improvement): 
EM: 78.282 (+0.757) 
F1: 81.069 (+0.663)

# BERT LARGE WITH NO ANSWER TUNING THRESHOLD OUR CS224N DEV SET
Submission successful
Scores (Improvement): 
EM: 79.138 (+0.856) 
F1: 81.599 (+0.530)

# BERT LARGE WITH MAX_SEQ LENGTH OF 512 instead of 384
Submission successful
Scores (Improvement): 
EM: 77.361 (-1.777) 
F1: 80.334 (-1.265)

# BERT LARGE WITH MAX_SEQ LENGTH OF 512 instead of 384 and no answer tuning
Submission successful
Scores (Improvement): 
EM: 79.121 (-0.016) 
F1: 81.585 (-0.014)

# BERT LARGE WITH MAX QUERY of 256, MAX_SEQ of 512, AND MAX ANSWER LEN OF 256
Submission successful
Scores (Improvement): 
EM: 77.279 (-1.859) 
F1: 80.275 (-1.324)

# BERT LARGE WITH MAX QUERY of 256, MAX_SEQ of 512, AND MAX ANSWER LEN OF 256 and no answer tuning
Submission successful
Scores (Improvement): 
EM: 79.056 (-0.082) 
F1: 81.551 (-0.048)

# BERT LARGE OPTIMIZED PARAMETERS
Submission successful
Scores (Improvement): 
EM: 73.511 (-5.627) 
F1: 77.272 (-4.327)

# BERT LARGE CNN without highway connection
Submission successful
Scores (Improvement): 
EM: 66.469 (-12.669) 
F1: 72.672 (-8.927)

# BERT LARGE CNN with highway connection
Submission successful
Scores (Improvement): 
EM: 76.259 (-2.879) 
F1: 79.656 (-1.943)

# BERT LARGE CNN with highway connection and no answer tuning
Submission successful
Scores (Improvement): 
EM: 77.772 (-1.366) 
F1: 81.047 (-0.552)





# count_lengths.py python file run flags/commands/arguments
python count_lengths.py --json_file /Users/diai/Documents/CS_224N_NLP/Project/QeRMIT/pytorch-pretrained-BERT/data/train_small.json


# count_lengths output
Number of examples: 130571.
Number of sequences that exceeded max_seq_len of 384 is 1625.
Number of queries that exceeded max_query_len of 64 is 0.
Number of answers that exceeded max_answer_len of 30 is 152.
Max seq length found was 512.
Max query length found was 63.
Max answer length found was 67.

export squad_dir=~/hugface/hugface/pytorch-pretrained-BERT/data

#normal <- I think this ran out of memory
python run_squad.py \
  --bert_model bert-base-uncased \
  --do_train \
  --do_predict \
  --do_lower_case \
  --train_file $squad_dir/train-v2.0.json \
  --predict_file $squad_dir/dev-v2.0.json \
  --train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /tmp/debug_squad/

# below is for using fp16
# more changes to accommodate smaller memory
python run_squad.py \
  --bert_model bert-large-uncased \
  --do_train \
  --do_predict \
  --do_lower_case \
  --train_file $squad_dir/train-v2.0.json \
  --predict_file $squad_dir/dev-v2.0.json \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /tmp/debug_squad/ \
  --train_batch_size 8 \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4




# for predicting on dev set given a certain model dir
export squad_dir=~/hugface/hugface/pytorch-pretrained-BERT/data
export model_dir=~/tmp/debug_squad/

python run_squad.py \
  --bert_model $model_dir \
  --do_predict \
  --do_lower_case \
  --predict_file $squad_dir/dev-v2.0.json \
  --doc_stride 128 \
  --output_dir $model_dir/output/ \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4

# for iterating on model test
export squad_dir=~/hugface/hugface/pytorch-pretrained-BERT/data
export model_dir=~/tmp/debug_squad/

python run_squad.py \
  --bert_model $model_dir \
  --do_predict \
  --do_lower_case \
  --predict_file $squad_dir/dev-v2.0.json \
  --doc_stride 128 \
  --output_dir $model_dir/output/ \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4 \
  --freeze_BERT_embed

# for iterating on training the model...
python run_squad.py \
  --bert_model $model_dir \
  --do_train \
  --do_predict \
  --do_lower_case \
  --train_file $squad_dir/train-v2.0.json \
  --predict_file $squad_dir/dev-v2.0.json \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir $model_dir/output/ \
  --train_batch_size 8 \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4
  --freeze_BERT_embed



*****************************CUSTOM scripts***********************************
0. Used to create baseline model for single layer
python run_squad.py   --bert_model bert-large-uncased   --do_train   --do_predict   --do_lower_case   --train_file $squad_dir/train-v2.0.json   --predict_file $squad_dir/dev-v2.0.json   --learning_rate 3e-5   --num_train_epochs 2   --max_seq_length 384   --doc_stride 128   --output_dir /tmp/debug_squad/   --train_batch_size 8   --fp16   --loss_scale 128   --version_2_with_negative   --gradient_accumulation_steps 4 --freeze_BERT_embed


0.1. Download file to local
scp -r  boss@40.80.149.205:/tmp/debug_squad/ /home/mrkchang/Documents/Stanford/CS224N/fromVM

0.2 Rename folder and move it in VM
export latest_model_dir=~/hugface/debug_squad_baseline_03032019

1. To create smaller dataset
export data_dir=/home/mrkchang/Documents/Stanford/CS224N/hugface/pytorch-pretrained-BERT/data
python data_manipulator.py  --json_file $data_dir/dev-v2.0.json   --output_dir $data_dir --create_dbg --threshold 0.1 --output_file dev_small.json

python data_manipulator.py  --json_file $data_dir/train-v2.0.json   --output_dir $data_dir --create_dbg --threshold 0.1 --output_file train_small.json

2. To create csv from json
export output_dir=/home/mrkchang/Documents/Stanford/CS224N/hugface/pytorch-pretrained-BERT/tmp/debug_squad_baseline_03032019

python json2csv.py --json_file $output_dir/predictions.json --output_dir $output_dir

Submitted:
EM: 77.526 (+77.526)
F1: 80.407 (+80.407)

optional. To dbg locally
export squad_dir=./data
export dbg_dir=./tmp/debug
python run_squad.py \
  --bert_model bert-base-uncased \
  --do_train \
  --do_predict \
  --do_lower_case \
  --train_file $squad_dir/dbg_sampled.json \
  --predict_file $squad_dir/dbg_sampled.json \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir $dbg_dir \
  --train_batch_size 8 \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4 \
  --freeze_BERT_embed
 
3. For debugging on new model...
tmux a -t returnOfTheKing
export squad_dir=~/hugface/hugface/pytorch-pretrained-BERT/data
export model_dir=~/hugface/debug_squad_baseline_03032019
export output_dir=~/hugface/debug_squad_baseline_03032019_cnn
python run_squad.py \
  --bert_model $model_dir \
  --do_train \
  --do_predict \
  --do_lower_case \
  --train_file $squad_dir/train-v2.0.json \
  --predict_file $squad_dir/dev-v2.0.json \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir $output_dir \
  --train_batch_size 8 \
  --fp16 \
  --loss_scale 128 \
  --version_2_with_negative \
  --gradient_accumulation_steps 4 \
  --freeze_BERT_embed \
  --new_model

4. After it's ready more get rid of --new_model --cache_example to save time <- preprocessing data
# TRAINING again
export squad_dir=~/hugface/hugface/pytorch-pretrained-BERT/data
export model_dir=~/hugface/debug_squad_baseline_03032019
export output_dir=~/hugface/debug_squad_baseline_03032019_cnn
python run_squad.py   --bert_model $model_dir   --do_train   --do_predict   --do_lower_case   --train_file $squad_dir/train-v2.0.json   --predict_file $squad_dir/dev-v2.0.json   --learning_rate 3e-5   --num_train_epochs 2   --max_seq_length 384   --doc_stride 128   --output_dir $output_dir   --train_batch_size 4   --fp16   --loss_scale 128   --version_2_with_negative   --gradient_accumulation_steps 4   --freeze_BERT_embed


